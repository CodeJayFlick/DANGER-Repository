import torch
from transformers import BertTokenizer, BertModel
from sklearn.metrics import accuracy_score
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import pandas as pd
import numpy as np

class AmazonReviewDataset(Dataset):
    def __init__(self, data, tokenizer, max_length):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        review_body = self.data.iloc[idx]['review_body']
        star_rating = float(self.data.iloc[idx]['star_rating']) - 1.0
        
        encoding = self.tokenizer.encode_plus(
            review_body,
            add_special_tokens=True,
            max_length=self.max_length,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(star_rating, dtype=torch.float)
        }

class BertFeaturizer:
    def __init__(self, tokenizer, max_length):
        self.tokenizer = tokenizer
        self.max_length = max_length

    def featurize(self, input_data):
        vocab = self.tokenizer.get_vocab()
        inputs = []
        labels = []

        for review_body in input_data['review_body']:
            tokens = self.tokenizer.tokenize(review_body.lower())
            if len(tokens) > self.max_length:
                tokens = tokens[:self.max_length]
            inputs.append([vocab['[CLS]']] + [vocab[token] for token in tokens] + [vocab['[SEP]']])
            labels.append(float(input_data[input_data.index == review_body]['star_rating']) - 1.0)

        return {'input_ids': np.array(inputs), 'labels': np.array(labels)}

def train(model, device, data_loader):
    model.train()
    total_loss = 0
    for batch in data_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
        loss_fn = torch.nn.CrossEntropyLoss()

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    return total_loss / len(data_loader)

def evaluate(model, device, data_loader):
    model.eval()
    total_correct = 0
    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            outputs = model(input_ids, attention_mask)
            _, predicted = torch.max(outputs.scores, 1)
            total_correct += (predicted == labels).sum().item()

    return accuracy_score(labels.cpu().numpy(), predicted.cpu().numpy())

def main():
    arguments = Arguments()
    if not arguments:
        return None

    model_urls = "https://resources.djl.ai/test-models/distilbert.zip"
    if Engine.getInstance().getEngineName() == 'PyTorch':
        model_urls = "https://resources.djl.ai/test-models/traced_distilbert_wikipedia_uncased.zip"

    criteria = Criteria.builder().optApplication(Application.NLP.WORD_EMBEDDING).setTypes(NDList, NDList).optModelUrls(model_urls).build()
    max_token_length = 64

    try:
        model = Model.newInstance("AmazonReviewRatingClassification")
        embedding = criteria.load_model()

        vocabulary = DefaultVocabulary.builder().addFromTextFile(embedding.getArtifact("vocab.txt")).optUnknownToken("[UNK]").build()
        tokenizer = BertFullTokenizer(vocabulary, True)
        dataset = get_dataset(arguments, tokenizer, max_token_length)

        datasets = dataset.random_split(7, 3)
        training_set = datasets[0]
        validation_set = datasets[1]

        model.set_block(get_block(embedding.new_predictor()))
        config = setup_training_config(arguments)
        try:
            trainer = model.new_trainer(config)
            trainer.set_metrics(Metrics())
            encoder_input_shape = Shape(arguments.get_batch_size(), max_token_length)

            trainer.initialize(encoder_input_shape)

            easy_train.fit(trainer, arguments.get_epoch(), training_set, validation_set)
            return trainer.get_training_result()
        except Exception as e:
            print(f"Error: {e}")
    finally:
        pass

def get_dataset(arguments, tokenizer, max_token_length):
    amazon_review = "https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz"
    padding_token = tokenizer.get_vocab().get_index("[PAD]")
    return AmazonReviewDataset(pd.read_csv(amazon_review), tokenizer, max_token_length)

def get_block(embedder):
    return SequentialBlock([
        add_freeze_layer(embedder),
        Linear(builder=set_units(768)),
        Activation::relu,
        Dropout(builder=opt_rate(0.2f)),
        Linear(builder=set_units(5))
    ])

def setup_training_config(arguments):
    output_dir = arguments.get_output_dir()
    listener = SaveModelTrainingListener(output_dir)
    return DefaultTrainingConfig(Loss.softmax_cross_entropy_loss()).add_evaluator(Accuracy())

if __name__ == "__main__":
    main()

