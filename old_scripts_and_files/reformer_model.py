# -*- coding: utf-8 -*-
"""Reformer_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mszNS63Fzx6TlZvato0h6yMtuW60h0fK
"""

import sys
sys.path.append('/content/drive/MyDrive/Reformer_Model_v3/DANGER-Repository-main')

import os
import pandas as pd
from sklearn.model_selection import train_test_split
import torch
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
import transformers
from transformers import ReformerModel, ReformerTokenizerFast
import get_model_training_data
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
import numpy as np

HUMAN_LABEL = 'human'
AI_LABEL = 'ai'

def load_dataframe():
  df = get_model_training_data.get_dataframe()

  print("Original Datagrame:")
  print(df)

  # Label processing for AI and Human Written code
  df['label'] = df['label'].str.lower().replace({'false': HUMAN_LABEL, 'true': AI_LABEL, 'no': HUMAN_LABEL, 'yes': AI_LABEL})

  print("\nDataframe after label processing:")
  print(df)

  # Dropping rows in which a label is NaN
  df = df.dropna(subset=['label'])

  # Mapping labels to numeric values
  df['msg_type'] = df['label'].map({HUMAN_LABEL: 0, AI_LABEL: 1})
  return df

os.chdir('/content/drive/MyDrive/Reformer_Model_v3/DANGER-Repository-main')
df = load_dataframe()
train_df, test_df = train_test_split(df, test_size = 0.2, random_state = 42)

print(df.head())

tokenizer = Tokenizer(num_words = 50000)
all_texts = train_df['code_sample'].tolist() + test_df['code_sample'].tolist()
tokenizer.fit_on_texts(all_texts)

from tensorflow.keras.preprocessing.sequence import pad_sequences

# Define chunk size
chunk_size = 32
max_length = 512

# Initialize tokenizer
tokenizer = Tokenizer(num_words=50000)

# Assuming the code samples are in the 'code' column
all_texts = train_df['code_sample'].tolist() + test_df['code_sample'].tolist()
tokenizer.fit_on_texts(all_texts)

# Function to create data loader from data
def create_dataloader(df):
    data = []

    df_reset = df.reset_index(drop = True)

    for i in range(0, len(df), chunk_size):
        chunk_texts = df.iloc[i:i+chunk_size-1, df_reset.columns.get_loc("code_sample")].tolist()
        chunk_labels = df.iloc[i:i+chunk_size-1, df_reset.columns.get_loc("msg_type")].tolist()

        # Convert texts to sequences of token IDs
        input_ids = tokenizer.texts_to_sequences(chunk_texts)

        input_ids = pad_sequences(input_ids, maxlen = max_length, padding = 'post', truncating = 'post')



        chunk = {
            "input_ids": input_ids,
            "labels": chunk_labels
        }
        data.append(chunk)
    return data

# Create train and test dataloaders
train_data = create_dataloader(train_df)
test_data = create_dataloader(test_df)

# Define custom Reformer Dataset class
class Reformer_Dataset(tf.keras.layers.Layer):
    def __init__(self, d_model=256, d_ff=512, max_length=512, vocab_size=50000, chunk_size=32):
        super(Reformer_Dataset, self).__init__()

        self.d_model = d_model
        self.d_ff = d_ff
        self.max_length = max_length
        self.chunk_size = chunk_size

        self.lstm = tf.keras.layers.LSTM(self.d_model)  # Using LSTM here for the example
        self.dense = tf.keras.layers.Dense(self.d_model)
        self.final_dense = tf.keras.layers.Dense(1)
        self.dropout = tf.keras.layers.Dropout(0.2)

    def call(self, inputs):
      inputs = tf.expand_dims(inputs, -1)

      # Assuming inputs are already tokenized
      #reformer_output = tf.keras.layers.LSTM(self.d_model)(inputs)  # Using LSTM here for the example
      #dense_output = tf.keras.layers.Dense(self.d_model)(reformer_output)
      #output = tf.keras.layers.Dense(1)(dense_output[:, 1, :])  # Taking the output of the second token
      #output = tf.keras.layers.Dense(1)(dense_output)

      reformer_output = self.lstm(inputs)
      reformer_output = self.dropout(reformer_output)
      dense_output = self.dense(reformer_output)
      output = self.final_dense(dense_output)
      return output

# Create Reformer Layer and Model
reformer_layer = Reformer_Dataset()

model = tf.keras.Sequential([reformer_layer])

# Compile the model
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-7))

# Training loop
for batch in train_data:
    # Convert lists of input_ids and labels into tensors
    input_tensor = tf.convert_to_tensor(batch["input_ids"], dtype=tf.int32)
    label_tensor = tf.convert_to_tensor(batch["labels"], dtype=tf.int32)

    # Create a TensorFlow dataset for batch processing
    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, label_tensor)).batch(chunk_size)

    # Iterate over the dataset to make predictions and calculate loss and accuracy
    for inputs, labels in dataset:
        with tf.GradientTape() as tape:
            # Forward pass
            predictions = model(inputs)
            # Loss calculation
            loss_value = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(labels, predictions)

        labels = tf.cast(labels, tf.int64)

        # Accuracy calculation
        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(predictions, axis=-1, output_type = tf.int64), labels), tf.float32))

        # Display loss and accuracy
        print(f"Loss: {loss_value.numpy()}")
        print(f"Accuracy: {accuracy.numpy():.4f}")